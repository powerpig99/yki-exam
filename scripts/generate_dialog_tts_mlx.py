#!/usr/bin/env python3
"""Generate per-turn TTS audio for YKI dialogues using Chatterbox (mlx-audio).

Reads fi_en_package.md for Finnish text, generates audio per turn with
3 distinct voices (narrator, Speaker A, Speaker B), concatenates into
merged.mp3, and writes manifest.json with per-segment timing.

Must run with: .venv_mlx_audio/bin/python3
"""
from __future__ import annotations

import argparse
import json
import os
import re
import shutil
import subprocess
import tempfile
import time
from pathlib import Path

os.environ.setdefault("TRANSFORMERS_VERBOSITY", "error")

from mlx_audio.tts.generate import generate_audio, load_model


# --- Regexes for parsing fi_en_package.md ---
CONTEXT_RE = re.compile(r"^\*\*FI Konteksti:\*\*\s*(.+)$")
FI_DIALOG_HEADER_RE = re.compile(r"^\*\*FI Koko mallidialogi:\*\*")
TURN_RE = re.compile(r"^-\s+\*\*([AB])\*\*:\s*(.+)$")


def ffmpeg_binary() -> str:
    preferred = Path("/opt/homebrew/opt/ffmpeg-full/bin/ffmpeg")
    if preferred.exists():
        return str(preferred)
    return shutil.which("ffmpeg") or ""


def ffprobe_duration_seconds(path: Path) -> float:
    preferred = Path("/opt/homebrew/opt/ffmpeg-full/bin/ffprobe")
    ffprobe = str(preferred) if preferred.exists() else (shutil.which("ffprobe") or "")
    if not ffprobe:
        return 0.0
    cmd = [
        ffprobe, "-v", "error",
        "-show_entries", "format=duration",
        "-of", "default=noprint_wrappers=1:nokey=1",
        str(path),
    ]
    try:
        out = subprocess.check_output(cmd, text=True).strip()
        return round(float(out), 3) if out else 0.0
    except Exception:
        return 0.0


def generate_silence(out_path: Path, duration_sec: float, sample_rate: int = 24000):
    """Generate a silent WAV file."""
    ffmpeg = ffmpeg_binary()
    cmd = [
        ffmpeg, "-y",
        "-f", "lavfi",
        "-i", f"anullsrc=r={sample_rate}:cl=mono",
        "-t", str(duration_sec),
        "-c:a", "pcm_s16le",
        str(out_path),
    ]
    subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)


def parse_fi_en_package(path: Path) -> tuple[str, list[tuple[str, str]]]:
    """Parse fi_en_package.md returning (context, [(speaker, text), ...])."""
    lines = path.read_text(encoding="utf-8").splitlines()

    context = ""
    turns: list[tuple[str, str]] = []
    in_fi_dialog = False

    for line in lines:
        stripped = line.strip()

        m = CONTEXT_RE.match(stripped)
        if m:
            context = m.group(1).strip()
            continue

        if FI_DIALOG_HEADER_RE.match(stripped):
            in_fi_dialog = True
            continue

        # Stop at EN section or next ** header
        if in_fi_dialog and (stripped.startswith("**EN ") or stripped.startswith("####")):
            in_fi_dialog = False
            continue

        if in_fi_dialog:
            m = TURN_RE.match(stripped)
            if m:
                turns.append((m.group(1), m.group(2).strip()))

    return context, turns


def concat_audio_files(files: list[Path], out_file: Path) -> None:
    """Concatenate audio files with ffmpeg."""
    ffmpeg = ffmpeg_binary()
    with tempfile.TemporaryDirectory(prefix="tts_concat_") as td:
        list_file = Path(td) / "concat_list.txt"
        with list_file.open("w", encoding="utf-8") as f:
            for p in files:
                p_escaped = str(p).replace("'", "'\\''")
                f.write(f"file '{p_escaped}'\n")

        out_ext = out_file.suffix.lower().lstrip(".")
        cmd = [
            ffmpeg, "-y",
            "-f", "concat", "-safe", "0",
            "-i", str(list_file),
            "-ar", "24000", "-ac", "1",
        ]
        if out_ext in ("wav", "wave"):
            cmd.extend(["-c:a", "pcm_s16le"])
        else:
            cmd.extend(["-c:a", "libmp3lame", "-b:a", "160k"])
        cmd.append(str(out_file))
        proc = subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE, text=True)
        if proc.returncode != 0:
            raise RuntimeError(f"ffmpeg concat failed: {(proc.stderr or '')[-500:]}")


def find_generated_wav(audio_dir: Path, prefix: str) -> Path:
    """Find the WAV file generated by Chatterbox for a given prefix.

    Chatterbox appends _000 (and possibly _001, etc.) to the prefix.
    If multiple chunks exist, concatenate them into one file.
    """
    pattern = sorted(audio_dir.glob(f"{prefix}_*.wav"))
    if not pattern:
        raise FileNotFoundError(f"No WAV files found for prefix {prefix} in {audio_dir}")
    if len(pattern) == 1:
        return pattern[0]
    # Multiple chunks: concat into single file
    merged = audio_dir / f"{prefix}.wav"
    concat_audio_files(pattern, merged)
    return merged


def generate_dialogue_audio(
    dialogue_dir: Path,
    model,
    voice_narrator_ref: str | None,
    voice_a_ref: str | None,
    voice_b_ref: str | None,
    pause_after_narrator: float = 1.0,
    pause_between_turns: float = 0.6,
) -> dict:
    """Generate TTS for one dialogue directory. Returns manifest dict."""

    pkg_path = dialogue_dir / "fi_en_package.md"
    if not pkg_path.exists():
        raise FileNotFoundError(f"No fi_en_package.md in {dialogue_dir}")

    context, turns = parse_fi_en_package(pkg_path)
    if not turns:
        raise ValueError(f"No turns found in {pkg_path}")

    audio_dir = dialogue_dir / "audio"
    audio_dir.mkdir(parents=True, exist_ok=True)

    dia_id = dialogue_dir.name
    segments: list[dict] = []
    turn_idx = 0

    # --- Narrator: context (no "Tilanne:" prefix) ---
    narrator_text = context
    narrator_prefix = f"turn_{turn_idx:03d}_narrator"
    print(f"  [narrator] {narrator_text[:70]}...")
    t0 = time.time()
    generate_audio(
        text=narrator_text,
        model=model,
        lang_code="fi",
        output_path=str(audio_dir),
        file_prefix=narrator_prefix,
        audio_format="wav",
        verbose=False,
        **({"ref_audio": voice_narrator_ref} if voice_narrator_ref else {}),
    )
    elapsed = time.time() - t0
    narrator_file = find_generated_wav(audio_dir, narrator_prefix)
    narrator_dur = ffprobe_duration_seconds(narrator_file)
    print(f"    {elapsed:.1f}s gen → {narrator_dur:.1f}s audio")

    segments.append({
        "type": "speech",
        "speaker": "narrator",
        "text_fi": narrator_text,
        "file": narrator_file.name,
        "duration_sec": narrator_dur,
    })
    segments.append({
        "type": "pause",
        "speaker": "",
        "text_fi": "",
        "file": "",
        "duration_sec": pause_after_narrator,
    })
    turn_idx += 1

    # --- Dialogue turns ---
    for speaker, text in turns:
        ref_audio = voice_a_ref if speaker == "A" else voice_b_ref
        prefix = f"turn_{turn_idx:03d}_{speaker}"
        label = f"[{speaker}]"
        print(f"  {label} {text[:70]}{'...' if len(text) > 70 else ''}")
        t0 = time.time()
        generate_audio(
            text=text,
            model=model,
            lang_code="fi",
            output_path=str(audio_dir),
            file_prefix=prefix,
            audio_format="wav",
            verbose=False,
            **({"ref_audio": ref_audio} if ref_audio else {}),
        )
        elapsed = time.time() - t0
        wav_file = find_generated_wav(audio_dir, prefix)
        dur = ffprobe_duration_seconds(wav_file)
        print(f"    {elapsed:.1f}s gen → {dur:.1f}s audio")

        segments.append({
            "type": "speech",
            "speaker": speaker,
            "text_fi": text,
            "file": wav_file.name,
            "duration_sec": dur,
        })
        segments.append({
            "type": "pause",
            "speaker": "",
            "text_fi": "",
            "file": "",
            "duration_sec": pause_between_turns,
        })
        turn_idx += 1

    # Remove trailing pause
    if segments and segments[-1]["type"] == "pause":
        segments.pop()

    # --- Generate silence WAVs for pauses ---
    silence_cache: dict[str, Path] = {}
    for seg in segments:
        if seg["type"] != "pause":
            continue
        dur_key = f"{seg['duration_sec']:.1f}"
        if dur_key not in silence_cache:
            sil_path = audio_dir / f"silence_{dur_key}s.wav"
            if not sil_path.exists():
                generate_silence(sil_path, seg["duration_sec"])
            silence_cache[dur_key] = sil_path
        seg["file"] = silence_cache[dur_key].name

    # --- Concatenate all segments ---
    concat_files = [audio_dir / seg["file"] for seg in segments]
    merged_path = audio_dir / "merged.mp3"
    print(f"  Concatenating {len(concat_files)} segments...")
    concat_audio_files(concat_files, merged_path)
    total_dur = ffprobe_duration_seconds(merged_path)
    print(f"  → merged.mp3 ({total_dur:.1f}s)")

    # --- Compute absolute timing ---
    t = 0.0
    for seg in segments:
        seg["start_sec"] = round(t, 3)
        seg["end_sec"] = round(t + seg["duration_sec"], 3)
        t += seg["duration_sec"]

    manifest = {
        "dialogue_id": dia_id,
        "model": "mlx-community/chatterbox-fp16",
        "voice_narrator_ref": voice_narrator_ref or "",
        "voice_a_ref": voice_a_ref or "",
        "voice_b_ref": voice_b_ref or "",
        "segments": segments,
        "total_duration_sec": round(total_dur, 3),
        "merged_file": "merged.mp3",
    }

    manifest_path = audio_dir / "manifest.json"
    manifest_path.write_text(
        json.dumps(manifest, ensure_ascii=False, indent=2) + "\n",
        encoding="utf-8",
    )
    print(f"  Manifest written: {manifest_path.name}")
    return manifest


def main():
    parser = argparse.ArgumentParser(
        description="Generate per-turn TTS audio for YKI dialogues using Chatterbox"
    )
    parser.add_argument(
        "--dialogue-dir", type=Path,
        help="Single dialogue directory to process",
    )
    parser.add_argument(
        "--dialogues-root", type=Path,
        default=Path("dialog_practice/dialogues"),
        help="Root directory containing all dialogue dirs",
    )
    parser.add_argument(
        "--only", type=str, default=None,
        help="Comma-separated dialogue IDs (e.g. 01_dia_03,02_dia_01)",
    )
    parser.add_argument(
        "--voice-narrator-ref", type=str,
        default="media/_mlx_test/fi_auto_ryan_000.wav",
        help="Reference audio for narrator (distinct from dialogue speakers)",
    )
    parser.add_argument(
        "--voice-a-ref", type=str, default=None,
        help="Reference audio for Speaker A (None = default Chatterbox voice)",
    )
    parser.add_argument(
        "--voice-b-ref", type=str,
        default="media/_mlx_test/fi_auto_ryan_000.wav",
        help="Reference audio for Speaker B (default: same as narrator for voice distinction)",
    )
    parser.add_argument(
        "--pause-narrator", type=float, default=1.0,
        help="Pause after narrator in seconds",
    )
    parser.add_argument(
        "--pause-turns", type=float, default=0.6,
        help="Pause between dialogue turns in seconds",
    )
    parser.add_argument(
        "--force", action="store_true",
        help="Regenerate even if manifest already exists",
    )
    args = parser.parse_args()

    # Determine which dialogues to process
    if args.dialogue_dir:
        dirs = [args.dialogue_dir]
    else:
        root = args.dialogues_root
        if args.only:
            ids = [x.strip() for x in args.only.split(",")]
            dirs = [root / i for i in ids]
        else:
            dirs = sorted(
                d for d in root.iterdir()
                if d.is_dir() and (d / "fi_en_package.md").exists()
            )

    if not dirs:
        print("No dialogue directories found.")
        return

    model_id = "mlx-community/chatterbox-fp16"
    print(f"Loading model: {model_id}")
    model = load_model(model_id)
    print("Model loaded.\n")

    total = len(dirs)
    done = 0
    skipped = 0
    errors = 0

    for i, d in enumerate(dirs, 1):
        dia_id = d.name
        manifest_path = d / "audio" / "manifest.json"
        if manifest_path.exists() and not args.force:
            print(f"[{i}/{total}] {dia_id} — skipping (manifest exists)")
            skipped += 1
            continue

        print(f"\n[{i}/{total}] {dia_id}")
        print(f"{'─' * 50}")
        try:
            generate_dialogue_audio(
                dialogue_dir=d,
                model=model,
                voice_narrator_ref=args.voice_narrator_ref,
                voice_a_ref=args.voice_a_ref,
                voice_b_ref=args.voice_b_ref,
                pause_after_narrator=args.pause_narrator,
                pause_between_turns=args.pause_turns,
            )
            done += 1
        except Exception as e:
            print(f"  ERROR: {e}")
            errors += 1

    print(f"\n{'=' * 50}")
    print(f"Done: {done} generated, {skipped} skipped, {errors} errors (of {total} total)")


if __name__ == "__main__":
    main()
